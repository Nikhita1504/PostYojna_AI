# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cA88IpGm8GdRQDVpjzMHxROzi_LckxtJ
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import random
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ All libraries imported successfully!")

def create_sample_district_csv():
    """Create a sample district CSV file to avoid FileNotFoundError"""
    sample_data = {
        'STATE': [
            'Andhra Pradesh', 'Andhra Pradesh', 'Andhra Pradesh', 'Andhra Pradesh',
            'Bihar', 'Bihar', 'Bihar', 'Bihar',
            'Gujarat', 'Gujarat', 'Gujarat', 'Gujarat',
            'Karnataka', 'Karnataka', 'Karnataka', 'Karnataka',
            'Maharashtra', 'Maharashtra', 'Maharashtra', 'Maharashtra',
            'Tamil Nadu', 'Tamil Nadu', 'Tamil Nadu', 'Tamil Nadu',
            'Uttar Pradesh', 'Uttar Pradesh', 'Uttar Pradesh', 'Uttar Pradesh',
            'West Bengal', 'West Bengal', 'West Bengal', 'West Bengal'
        ],
        'DISTRICT': [
            'Anantapur', 'Chittoor', 'East Godavari', 'Guntur',
            'Araria', 'Arwal', 'Aurangabad', 'Banka',
            'Ahmedabad', 'Amreli', 'Anand', 'Bharuch',
            'Bagalkot', 'Bangalore Rural', 'Bangalore Urban', 'Belgaum',
            'Ahmednagar', 'Akola', 'Amravati', 'Aurangabad',
            'Ariyalur', 'Chennai', 'Coimbatore', 'Cuddalore',
            'Agra', 'Aligarh', 'Allahabad', 'Ambedkar Nagar',
            'Bankura', 'Bardhaman', 'Birbhum', 'Cooch Behar'
        ]
    }

    df = pd.DataFrame(sample_data)
    df.to_csv('List-Statewise-Districts.csv', index=False)
    print("‚úÖ Sample district CSV file created!")
    print(f"Created {len(df)} sample districts across {df['STATE'].nunique()} states")
    return df

class POSBSchemePredictor:
    """
    POSB (Post Office Savings Bank) Scheme Predictor
    Predicts potential for various post office savings schemes based on district demographics
    """

    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.label_encoders = {}
        self.data = None

        # List of POSB schemes
        self.schemes = [
            "Post Office Monthly Income Scheme",
            "Senior Citizen Savings Scheme",
            "Public Provident Fund",
            "Sukanya Samriddhi Yojana",
            "Postal Life Insurance",
            "Rural Postal Life Insurance",
            "National Savings Certificate",
            "Kisan Vikas Patra",
            "Mahila Samman Savings Certificate"
        ]

        # Load district data from CSV
        self.district_mapping = self._load_district_mapping()

    def _load_district_mapping(self):
        """Load district names from the CSV file"""
        try:
            # Create CSV if it doesn't exist
            try:
                df = pd.read_csv('List-Statewise-Districts.csv')
            except FileNotFoundError:
                df = create_sample_district_csv()

            # Clean column names
            df.columns = df.columns.str.strip().str.upper()

            # Check if required columns exist
            if 'DISTRICT' not in df.columns or 'STATE' not in df.columns:
                print(f"Warning: Required columns 'DISTRICT' and 'STATE' not found.")
                return self._get_fallback_districts()

            # Clean the data
            df = df.dropna(subset=['DISTRICT', 'STATE'])
            df = df[(df['DISTRICT'].str.strip() != '') & (df['STATE'].str.strip() != '')]

            # Create state-wise district mapping
            district_mapping = {}
            for _, row in df.iterrows():
                state = row['STATE'].strip()
                district = row['DISTRICT'].strip()

                if state not in district_mapping:
                    district_mapping[state] = []
                district_mapping[state].append(district)

            print(f"‚úÖ Loaded districts for {len(district_mapping)} states")
            return district_mapping

        except Exception as e:
            print(f"‚ö†Ô∏è Error reading CSV: {str(e)}. Using fallback data.")
            return self._get_fallback_districts()

    def _get_fallback_districts(self):
        """Fallback district mapping if CSV is not available"""
        return {
            'Andhra Pradesh': ['Anantapur', 'Chittoor', 'East Godavari', 'Guntur'],
            'Bihar': ['Araria', 'Arwal', 'Aurangabad', 'Banka'],
            'Gujarat': ['Ahmedabad', 'Amreli', 'Anand', 'Bharuch'],
            'Karnataka': ['Bagalkot', 'Bangalore Rural', 'Bangalore Urban', 'Belgaum'],
            'Maharashtra': ['Ahmednagar', 'Akola', 'Amravati', 'Aurangabad'],
            'Tamil Nadu': ['Ariyalur', 'Chennai', 'Coimbatore', 'Cuddalore'],
            'Uttar Pradesh': ['Agra', 'Aligarh', 'Allahabad', 'Ambedkar Nagar'],
            'West Bengal': ['Bankura', 'Bardhaman', 'Birbhum', 'Cooch Behar']
        }

    def generate_synthetic_data(self):
        """Generate synthetic demographic and economic data for districts"""
        # State-wise district counts (simplified for demo)
        states_districts = {
            'Andhra Pradesh': 24, 'Arunachal Pradesh': 16, 'Assam': 27, 'Bihar': 38,
            'Chhattisgarh': 16, 'Goa': 2, 'Gujarat': 26, 'Haryana': 21,
            'Himachal Pradesh': 12, 'Jharkhand': 22, 'Karnataka': 28, 'Kerala': 14,
            'Madhya Pradesh': 48, 'Maharashtra': 32, 'Manipur': 10, 'Meghalaya': 7,
            'Mizoram': 9, 'Nagaland': 11, 'Odisha': 30, 'Punjab': 20,
            'Rajasthan': 33, 'Sikkim': 4, 'Tamil Nadu': 42, 'Tripura': 4,
            'Uttar Pradesh': 70, 'Uttarakhand': 13, 'West Bengal': 19
        }

        data = []
        district_id = 1

        print("üîÑ Generating synthetic data...")

        for state, district_count in states_districts.items():
            # Get available districts for this state
            available_districts = self.district_mapping.get(state, [])

            for i in range(district_count):
                # Use real district name if available
                if i < len(available_districts):
                    district_name = available_districts[i]
                else:
                    district_name = f"{state}_District_{i+1}"

                # Generate realistic demographic data
                total_population = random.randint(500000, 3000000)

                # Age distribution
                child_percent = random.uniform(25, 35)
                youth_percent = random.uniform(30, 40)
                adult_percent = random.uniform(25, 35)
                senior_percent = max(0, 100 - child_percent - youth_percent - adult_percent)

                # Gender distribution
                male_percent = random.uniform(48, 53)
                female_percent = 100 - male_percent

                # Economic indicators
                literacy_rate = random.uniform(60, 95)
                urban_percent = random.uniform(10, 80)
                rural_percent = 100 - urban_percent

                # Occupation distribution
                farmer_percent = random.uniform(20, 60)
                service_holder_percent = random.uniform(15, 40)
                business_percent = random.uniform(10, 30)
                others_percent = max(0, 100 - farmer_percent - service_holder_percent - business_percent)

                # Economic indicators
                avg_income = random.uniform(150000, 800000)
                bank_penetration = random.uniform(40, 85)
                post_offices_count = random.randint(50, 500)

                district_data = {
                    'district_id': district_id,
                    'state': state,
                    'district': district_name,
                    'total_population': total_population,
                    'child_percent': child_percent,
                    'youth_percent': youth_percent,
                    'adult_percent': adult_percent,
                    'senior_percent': senior_percent,
                    'male_percent': male_percent,
                    'female_percent': female_percent,
                    'literacy_rate': literacy_rate,
                    'urban_percent': urban_percent,
                    'rural_percent': rural_percent,
                    'farmer_percent': farmer_percent,
                    'service_holder_percent': service_holder_percent,
                    'business_percent': business_percent,
                    'others_percent': others_percent,
                    'avg_income': avg_income,
                    'bank_penetration': bank_penetration,
                    'post_offices_count': post_offices_count
                }

                # Generate scheme-specific targets
                district_data.update(self._generate_scheme_targets(district_data))
                data.append(district_data)
                district_id += 1

        # Create DataFrame
        df = pd.DataFrame(data)
        print(f"‚úÖ Generated synthetic data for {len(data)} districts")
        return df

    def _generate_scheme_targets(self, district_data):
        """Generate realistic targets for each scheme based on demographics"""
        pop = district_data['total_population']
        targets = {}

        # Post Office Monthly Income Scheme
        pomis_factor = (district_data['senior_percent'] * 0.6 +
                       district_data['service_holder_percent'] * 0.4 +
                       district_data['urban_percent'] * 0.3) / 100
        targets['Post Office Monthly Income Scheme'] = int(pop * pomis_factor * random.uniform(0.15, 0.25))

        # Senior Citizen Savings Scheme
        scss_factor = district_data['senior_percent'] / 100
        targets['Senior Citizen Savings Scheme'] = int(pop * scss_factor * random.uniform(0.45, 0.65))

        # Public Provident Fund
        ppf_factor = (district_data['service_holder_percent'] * 0.7 +
                     district_data['business_percent'] * 0.5 +
                     district_data['urban_percent'] * 0.3) / 100
        targets['Public Provident Fund'] = int(pop * ppf_factor * random.uniform(0.25, 0.40))

        # Sukanya Samriddhi Yojana
        girl_child_factor = (district_data['child_percent'] * district_data['female_percent'] / 200) / 100
        targets['Sukanya Samriddhi Yojana'] = int(pop * girl_child_factor * random.uniform(0.8, 1.2))

        # Postal Life Insurance
        pli_factor = (district_data['service_holder_percent'] * 0.6 +
                     district_data['business_percent'] * 0.4 +
                     district_data['urban_percent'] * 0.3) / 100
        targets['Postal Life Insurance'] = int(pop * pli_factor * random.uniform(0.20, 0.35))

        # Rural Postal Life Insurance
        rpli_factor = (district_data['rural_percent'] * 0.6 +
                      district_data['farmer_percent'] * 0.5) / 100
        targets['Rural Postal Life Insurance'] = int(pop * rpli_factor * random.uniform(0.18, 0.28))

        # National Savings Certificate
        nsc_factor = (district_data['service_holder_percent'] * 0.5 +
                     district_data['business_percent'] * 0.4 +
                     district_data['literacy_rate'] * 0.02) / 100
        targets['National Savings Certificate'] = int(pop * nsc_factor * random.uniform(0.15, 0.25))

        # Kisan Vikas Patra
        kvp_factor = (district_data['farmer_percent'] * 0.7 +
                     district_data['rural_percent'] * 0.4) / 100
        targets['Kisan Vikas Patra'] = int(pop * kvp_factor * random.uniform(0.25, 0.40))

        # Mahila Samman Savings Certificate
        mssc_factor = (district_data['female_percent'] * 0.6 +
                      district_data['service_holder_percent'] * 0.3 +
                      district_data['literacy_rate'] * 0.02) / 100
        targets['Mahila Samman Savings Certificate'] = int(pop * mssc_factor * random.uniform(0.30, 0.45))

        return targets

    def prepare_features(self, df):
        """Prepare features for ML model"""
        feature_columns = [
            'total_population', 'child_percent', 'youth_percent', 'adult_percent',
            'senior_percent', 'male_percent', 'female_percent', 'literacy_rate',
            'urban_percent', 'rural_percent', 'farmer_percent', 'service_holder_percent',
            'business_percent', 'others_percent', 'avg_income', 'bank_penetration',
            'post_offices_count'
        ]

        # Encode state variable
        if 'state' in df.columns:
            if 'state' not in self.label_encoders:
                self.label_encoders['state'] = LabelEncoder()
                df = df.copy()  # Create a copy to avoid SettingWithCopyWarning
                df['state_encoded'] = self.label_encoders['state'].fit_transform(df['state'])
            else:
                df = df.copy()
                df['state_encoded'] = self.label_encoders['state'].transform(df['state'])
            feature_columns.append('state_encoded')

        return df[feature_columns]

    def train_models(self, df):
        """Train ML models for each scheme"""
        X = self.prepare_features(df)

        print("üöÄ Training models for each POSB scheme...")

        for scheme in self.schemes:
            print(f"  Training model for {scheme}...")

            y = df[scheme]

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # Train Random Forest model
            model = RandomForestRegressor(
                n_estimators=100,
                random_state=42,
                max_depth=10,
                min_samples_split=5
            )
            model.fit(X_train_scaled, y_train)

            # Evaluate
            y_pred = model.predict(X_test_scaled)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            print(f"    MSE: {mse:.2f}, R2: {r2:.3f}")

            # Store model and scaler
            self.models[scheme] = model
            self.scalers[scheme] = scaler

        print("‚úÖ All models trained successfully!")

    def save_model(self, filepath='posb_model.joblib'):
        """Save trained models"""
        model_data = {
            'models': self.models,
            'scalers': self.scalers,
            'label_encoders': self.label_encoders,
            'schemes': self.schemes
        }
        joblib.dump(model_data, filepath)
        print(f"üíæ Model saved to {filepath}")

    def load_model(self, filepath='posb_model.joblib'):
        """Load trained models"""
        try:
            model_data = joblib.load(filepath)
            self.models = model_data['models']
            self.scalers = model_data['scalers']
            self.label_encoders = model_data['label_encoders']
            self.schemes = model_data['schemes']
            print(f"üìÅ Model loaded from {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå Error loading model: {str(e)}")
            return False

    def predict_scheme_potential(self, district_data):
        """Predict potential for all schemes for a given district"""
        # Convert to DataFrame if it's a dictionary
        if isinstance(district_data, dict):
            df = pd.DataFrame([district_data])
        else:
            df = district_data.copy()

        X = self.prepare_features(df)

        predictions = {}
        probabilities = {}

        total_population = district_data['total_population'] if isinstance(district_data, dict) else district_data['total_population'].iloc[0]

        # Calculate maximum potential accounts
        max_potential_accounts = self._calculate_max_potential_accounts(district_data)

        for scheme in self.schemes:
            if scheme not in self.models or scheme not in self.scalers:
                print(f"‚ö†Ô∏è Model not found for scheme: {scheme}")
                predictions[scheme] = 0
                probabilities[scheme] = 0.0
                continue

            try:
                # Scale features
                X_scaled = self.scalers[scheme].transform(X)

                # Predict expected accounts
                pred = self.models[scheme].predict(X_scaled)[0]
                predictions[scheme] = max(0, int(pred))

                # Calculate probability
                max_potential = max_potential_accounts[scheme]
                if max_potential > 0:
                    probability = min((predictions[scheme] / max_potential) * 100, 100)
                else:
                    probability = 0

                probabilities[scheme] = round(probability, 2)
            except Exception as e:
                print(f"‚ö†Ô∏è Error predicting for {scheme}: {str(e)}")
                predictions[scheme] = 0
                probabilities[scheme] = 0.0

        return predictions, probabilities

    def _calculate_max_potential_accounts(self, district_data):
        """Calculate maximum potential accounts for each scheme"""
        if isinstance(district_data, dict):
            pop = district_data['total_population']
        else:
            pop = district_data['total_population'].iloc[0] if hasattr(district_data['total_population'], 'iloc') else district_data['total_population']
        
        max_potential = {}

        # Helper function to get value safely
        def get_value(key):
            if isinstance(district_data, dict):
                return district_data[key]
            else:
                val = district_data[key]
                return val.iloc[0] if hasattr(val, 'iloc') else val

        # Calculate max potential for each scheme based on target demographics
        senior_pop = int(pop * get_value('senior_percent') / 100)
        service_holders = int(pop * get_value('service_holder_percent') / 100)
        business_people = int(pop * get_value('business_percent') / 100)
        rural_pop = int(pop * get_value('rural_percent') / 100)
        farmers = int(pop * get_value('farmer_percent') / 100)

        max_potential['Post Office Monthly Income Scheme'] = senior_pop + int(pop * get_value('adult_percent') * 0.3 / 100)
        max_potential['Senior Citizen Savings Scheme'] = senior_pop
        max_potential['Public Provident Fund'] = service_holders + business_people
        max_potential['Sukanya Samriddhi Yojana'] = int(pop * get_value('child_percent') * get_value('female_percent') / 10000 * 0.8)
        max_potential['Postal Life Insurance'] = service_holders + business_people + int(pop * get_value('others_percent') * 0.5 / 100)
        max_potential['Rural Postal Life Insurance'] = int(rural_pop * 0.6)
        max_potential['National Savings Certificate'] = service_holders + business_people
        max_potential['Kisan Vikas Patra'] = farmers + int(rural_pop * 0.4)
        max_potential['Mahila Samman Savings Certificate'] = int(pop * get_value('female_percent') * (get_value('youth_percent') + get_value('adult_percent')) / 10000)

        return max_potential

    def get_district_analysis(self, state, district_id=None):
        """Get comprehensive analysis for a district"""
        if self.data is None:
            raise ValueError("No data available. Please generate or load data first.")

        print(f"Looking for state: '{state}'")
        
        # Check if data is empty
        if len(self.data) == 0:
            raise ValueError("Dataset is empty. Please generate data first.")

        # Filter data for the state (case-insensitive)
        state_data = self.data[self.data['state'].str.lower() == state.lower()]

        if len(state_data) == 0:
            # Try partial matching
            partial_matches = self.data[self.data['state'].str.contains(state, case=False, na=False)]
            if len(partial_matches) > 0:
                state_data = partial_matches
                print(f"Using partial match for state: {state}")
            else:
                available_states = list(self.data['state'].unique())
                raise ValueError(f"No data found for state: '{state}'. Available states: {available_states}")

        print(f"Found {len(state_data)} districts for state: {state}")

        # Handle district selection
        if district_id:
            # Check if district_id is actually a district name
            if isinstance(district_id, str):
                # Try to find by district name
                district_match = state_data[state_data['district'].str.lower() == district_id.lower()]
                if len(district_match) > 0:
                    district_data = district_match.iloc[0].to_dict()
                    print(f"Found district by name: {district_id}")
                else:
                    # Try partial matching for district name
                    partial_district_match = state_data[state_data['district'].str.contains(district_id, case=False, na=False)]
                    if len(partial_district_match) > 0:
                        district_data = partial_district_match.iloc[0].to_dict()
                        print(f"Found district by partial match: {district_id}")
                    else:
                        # Use first district if no match found
                        if len(state_data) > 0:
                            district_data = state_data.iloc[0].to_dict()
                            available_districts = list(state_data['district'].unique())
                            print(f"District '{district_id}' not found. Using first available district. Available districts: {available_districts}")
                        else:
                            raise ValueError(f"No districts found for state: {state}")
            else:
                # Numeric district_id
                district_match = state_data[state_data['district_id'] == district_id]
                if len(district_match) > 0:
                    district_data = district_match.iloc[0].to_dict()
                else:
                    if len(state_data) > 0:
                        district_data = state_data.iloc[0].to_dict()
                        print(f"District ID {district_id} not found. Using first available district.")
                    else:
                        raise ValueError(f"No districts found for state: {state}")
        else:
            # Use first district of the state
            if len(state_data) > 0:
                district_data = state_data.iloc[0].to_dict()
                print(f"No district specified. Using first district: {district_data['district']}")
            else:
                raise ValueError(f"No districts found for state: {state}")

        predictions, probabilities = self.predict_scheme_potential(district_data)
        max_potential_accounts = self._calculate_max_potential_accounts(district_data)

        # Rank schemes by potential
        scheme_ranking = sorted(predictions.items(), key=lambda x: x[1], reverse=True)

        analysis = {
            'district_info': {
                'state': district_data['state'],
                'district': district_data['district'],
                'district_id': district_data['district_id'],
                'total_population': district_data['total_population'],
                'demographics': {
                    'children': f"{district_data['child_percent']:.1f}%",
                    'youth': f"{district_data['youth_percent']:.1f}%",
                    'adults': f"{district_data['adult_percent']:.1f}%",
                    'seniors': f"{district_data['senior_percent']:.1f}%",
                    'male': f"{district_data['male_percent']:.1f}%",
                    'female': f"{district_data['female_percent']:.1f}%"
                },
                'economic_profile': {
                    'literacy_rate': f"{district_data['literacy_rate']:.1f}%",
                    'urban_population': f"{district_data['urban_percent']:.1f}%",
                    'farmers': f"{district_data['farmer_percent']:.1f}%",
                    'service_holders': f"{district_data['service_holder_percent']:.1f}%",
                    'business_people': f"{district_data['business_percent']:.1f}%",
                    'avg_income': f"‚Çπ{district_data['avg_income']:,.0f}",
                    'bank_penetration': f"{district_data['bank_penetration']:.1f}%"
                }
            },
            'scheme_predictions': predictions,
            'scheme_probabilities': probabilities,
            'max_potential_accounts': max_potential_accounts,
            'top_schemes': scheme_ranking[:5],
            'debug_info': {
                'requested_state': state,
                'requested_district': district_id,
                'found_state': district_data['state'],
                'found_district': district_data['district'],
                'available_states': list(self.data['state'].unique()),
                'available_districts_in_state': list(state_data['district'].unique())
            }
        }

        return analysis

# Initialize and setup function
def initialize_predictor():
    """Initialize the predictor with data and trained models"""
    print("\n" + "="*60)
    print("üöÄ INITIALIZING POSB SCHEME PREDICTOR")
    print("="*60)

    # Initialize predictor
    predictor = POSBSchemePredictor()

    # Try to load existing model first
    if not predictor.load_model('posb_predictor_model.joblib'):
        print("No existing model found. Training new model...")
        
        # Generate synthetic data
        print("\nGenerating synthetic data for districts...")
        df = predictor.generate_synthetic_data()
        predictor.data = df

        print(f"‚úÖ Generated data for {len(df)} districts across {df['state'].nunique()} states")
        print(f"üìä Data shape: {df.shape}")

        # Save synthetic data
        df.to_csv('synthetic_data.csv', index=False)
        print("üíæ Synthetic data saved to 'synthetic_data.csv'")

        # Train models
        print("\n" + "="*60)
        print("ü§ñ TRAINING MACHINE LEARNING MODELS")
        print("="*60)

        predictor.train_models(df)
        predictor.save_model('posb_predictor_model.joblib')
    else:
        # Load existing data if available
        try:
            predictor.data = pd.read_csv('synthetic_data.csv')
            print(f"üìÅ Loaded existing synthetic data: {predictor.data.shape}")
        except FileNotFoundError:
            print("No existing data found. Generating new data...")
            df = predictor.generate_synthetic_data()
            predictor.data = df
            df.to_csv('synthetic_data.csv', index=False)

    print("\n" + "="*60)
    print("‚úÖ POSB SCHEME PREDICTOR READY!")
    print("="*60)
    
    return predictor

# If running as main script
if __name__ == "__main__":
    # Initialize predictor
    predictor = initialize_predictor()
    
    # Run sample analysis
    print("\n" + "="*60)
    print("üìä SAMPLE ANALYSIS - TAMIL NADU DISTRICT")
    print("="*60)

    try:
        analysis = predictor.get_district_analysis('Tamil Nadu')

        print(f"\nüèòÔ∏è  District: {analysis['district_info']['district']}")
        print(f"üó∫Ô∏è  State: {analysis['district_info']['state']}")
        print(f"üë• Population: {analysis['district_info']['total_population']:,}")

        print(f"\nüìä Demographics:")
        for key, value in analysis['district_info']['demographics'].items():
            print(f"   {key.title()}: {value}")

        print(f"\nüíº Economic Profile:")
        for key, value in analysis['district_info']['economic_profile'].items():
            print(f"   {key.replace('_', ' ').title()}: {value}")

        print(f"\nüèÜ Top 5 Recommended Schemes:")
        for i, (scheme, potential) in enumerate(analysis['top_schemes'], 1):
            probability = analysis['scheme_probabilities'][scheme]
            print(f"   {i}. {scheme}")
            print(f"      Expected Accounts: {potential:,}")
            print(f"      Success Probability: {probability:.2f}%")
            print()

    except Exception as e:
        print(f"‚ùå Error in analysis: {str(e)}")
        import traceback
        traceback.print_exc()

    print("\n" + "="*60)
    print("üí° Usage Tips:")
    print("   - Use predictor.get_district_analysis('StateName') for analysis")
    print("   - Use predictor.data to access the generated dataset")
    print("   - Models are saved as 'posb_predictor_model.joblib'")
    print("="*60)

